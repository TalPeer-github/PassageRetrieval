{
 "cells": [
  {
   "cell_type": "code",
   "id": "f953492d-8b47-4caf-ba50-1338ec25c3bc",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "import statsmodels as sm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import faiss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.decomposition import LatentDirichletAllocation, PCA\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('punkt')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T15:03:33.464571Z",
     "start_time": "2025-02-08T15:03:33.459762Z"
    }
   },
   "cell_type": "code",
   "source": "import pandas as pd",
   "id": "3bbe5cbefd6b96e4",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T15:06:24.145541Z",
     "start_time": "2025-02-08T15:06:24.120841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "passages_df = pd.read_csv('data/passages_df.csv')\n",
    "passages_df['len_content'] = passages_df['content'].apply(lambda x: len(x))\n",
    "passages_df"
   ],
   "id": "6cab7482207a2942",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                str_idx  chapter_idx  passage_idx  \\\n",
       "0           CHAPTER ONE            1            0   \n",
       "1           CHAPTER ONE            1            1   \n",
       "2           CHAPTER ONE            1            2   \n",
       "3           CHAPTER ONE            1            3   \n",
       "4           CHAPTER ONE            1            4   \n",
       "...                 ...          ...          ...   \n",
       "3008  CHAPTER SEVENTEEN           17          216   \n",
       "3009  CHAPTER SEVENTEEN           17          217   \n",
       "3010  CHAPTER SEVENTEEN           17          218   \n",
       "3011  CHAPTER SEVENTEEN           17          219   \n",
       "3012  CHAPTER SEVENTEEN           17          220   \n",
       "\n",
       "                                                content  len_content  \n",
       "0     Mr. and Mrs. Dursley, of number four, Privet D...          262  \n",
       "1     Mr. Dursley was the director of a firm called ...          454  \n",
       "2     The Dursleys had everything they wanted, but t...          748  \n",
       "3     When Mr. and Mrs. Dursley woke up on the dull,...          377  \n",
       "4     None of them noticed a large, tawny owl flutte...           64  \n",
       "...                                                 ...          ...  \n",
       "3008  Harry hung back for a last word with Ron and H...           54  \n",
       "3009                   \"See you over the summer, then.\"           32  \n",
       "3010  \"Hope you have -- er -- a good holiday,\" said ...          139  \n",
       "3011  \"Oh, I will,\" said Harry, and they were surpri...          212  \n",
       "3012                                           THE END             8  \n",
       "\n",
       "[3013 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>str_idx</th>\n",
       "      <th>chapter_idx</th>\n",
       "      <th>passage_idx</th>\n",
       "      <th>content</th>\n",
       "      <th>len_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHAPTER ONE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Mr. and Mrs. Dursley, of number four, Privet D...</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHAPTER ONE</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mr. Dursley was the director of a firm called ...</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHAPTER ONE</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>The Dursleys had everything they wanted, but t...</td>\n",
       "      <td>748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHAPTER ONE</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>When Mr. and Mrs. Dursley woke up on the dull,...</td>\n",
       "      <td>377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHAPTER ONE</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>None of them noticed a large, tawny owl flutte...</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3008</th>\n",
       "      <td>CHAPTER SEVENTEEN</td>\n",
       "      <td>17</td>\n",
       "      <td>216</td>\n",
       "      <td>Harry hung back for a last word with Ron and H...</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3009</th>\n",
       "      <td>CHAPTER SEVENTEEN</td>\n",
       "      <td>17</td>\n",
       "      <td>217</td>\n",
       "      <td>\"See you over the summer, then.\"</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3010</th>\n",
       "      <td>CHAPTER SEVENTEEN</td>\n",
       "      <td>17</td>\n",
       "      <td>218</td>\n",
       "      <td>\"Hope you have -- er -- a good holiday,\" said ...</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3011</th>\n",
       "      <td>CHAPTER SEVENTEEN</td>\n",
       "      <td>17</td>\n",
       "      <td>219</td>\n",
       "      <td>\"Oh, I will,\" said Harry, and they were surpri...</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3012</th>\n",
       "      <td>CHAPTER SEVENTEEN</td>\n",
       "      <td>17</td>\n",
       "      <td>220</td>\n",
       "      <td>THE END</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3013 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T15:14:02.660640Z",
     "start_time": "2025-02-08T15:14:02.619833Z"
    }
   },
   "cell_type": "code",
   "source": "passages_df.groupby('chapter_idx').len_content.describe()#.mean()",
   "id": "5a8a220054a4c67a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             count        mean         std  min    25%    50%     75%     max\n",
       "chapter_idx                                                                  \n",
       "1            109.0  235.743119  208.664510  1.0  72.00  178.0  328.00   982.0\n",
       "2            102.0  185.862745  192.909615  1.0  54.25  116.0  228.75   913.0\n",
       "3            127.0  167.653543  149.563936  1.0  64.50  132.0  245.50   999.0\n",
       "4            147.0  136.829932  133.150413  1.0  43.50   92.0  186.50   692.0\n",
       "5            281.0  132.128114  126.777750  1.0  44.00   88.0  182.00   759.0\n",
       "6            286.0  120.402098  117.679564  1.0  39.00   80.0  164.50   753.0\n",
       "7            183.0  136.404372  152.424149  1.0  32.00   79.0  170.50   798.0\n",
       "8             91.0  187.384615  179.208182  1.0  72.00  133.0  239.00   862.0\n",
       "9            207.0  134.487923  123.816018  1.0  59.50   96.0  168.00   858.0\n",
       "10           160.0  149.287500  119.894523  1.0  59.75  114.0  213.00   695.0\n",
       "11           139.0  136.223022  147.336745  1.0  44.50   88.0  173.50  1071.0\n",
       "12           205.0  148.063415  141.953841  1.0  48.00   98.0  217.00   866.0\n",
       "13           120.0  150.550000  118.675807  1.0  60.00  117.5  199.75   533.0\n",
       "14           137.0  140.773723  109.399766  1.0  55.00  114.0  180.00   500.0\n",
       "15           186.0  153.838710  119.103797  1.0  69.00  119.5  194.75   757.0\n",
       "16           312.0  114.810897  104.439244  1.0  45.00   80.0  152.25   774.0\n",
       "17           221.0  138.647059  137.051126  7.0  42.00   87.0  196.00   693.0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chapter_idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109.0</td>\n",
       "      <td>235.743119</td>\n",
       "      <td>208.664510</td>\n",
       "      <td>1.0</td>\n",
       "      <td>72.00</td>\n",
       "      <td>178.0</td>\n",
       "      <td>328.00</td>\n",
       "      <td>982.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.0</td>\n",
       "      <td>185.862745</td>\n",
       "      <td>192.909615</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54.25</td>\n",
       "      <td>116.0</td>\n",
       "      <td>228.75</td>\n",
       "      <td>913.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>127.0</td>\n",
       "      <td>167.653543</td>\n",
       "      <td>149.563936</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.50</td>\n",
       "      <td>132.0</td>\n",
       "      <td>245.50</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>147.0</td>\n",
       "      <td>136.829932</td>\n",
       "      <td>133.150413</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43.50</td>\n",
       "      <td>92.0</td>\n",
       "      <td>186.50</td>\n",
       "      <td>692.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>281.0</td>\n",
       "      <td>132.128114</td>\n",
       "      <td>126.777750</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.00</td>\n",
       "      <td>88.0</td>\n",
       "      <td>182.00</td>\n",
       "      <td>759.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>286.0</td>\n",
       "      <td>120.402098</td>\n",
       "      <td>117.679564</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>80.0</td>\n",
       "      <td>164.50</td>\n",
       "      <td>753.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>183.0</td>\n",
       "      <td>136.404372</td>\n",
       "      <td>152.424149</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.00</td>\n",
       "      <td>79.0</td>\n",
       "      <td>170.50</td>\n",
       "      <td>798.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>91.0</td>\n",
       "      <td>187.384615</td>\n",
       "      <td>179.208182</td>\n",
       "      <td>1.0</td>\n",
       "      <td>72.00</td>\n",
       "      <td>133.0</td>\n",
       "      <td>239.00</td>\n",
       "      <td>862.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>207.0</td>\n",
       "      <td>134.487923</td>\n",
       "      <td>123.816018</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59.50</td>\n",
       "      <td>96.0</td>\n",
       "      <td>168.00</td>\n",
       "      <td>858.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>160.0</td>\n",
       "      <td>149.287500</td>\n",
       "      <td>119.894523</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59.75</td>\n",
       "      <td>114.0</td>\n",
       "      <td>213.00</td>\n",
       "      <td>695.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>139.0</td>\n",
       "      <td>136.223022</td>\n",
       "      <td>147.336745</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.50</td>\n",
       "      <td>88.0</td>\n",
       "      <td>173.50</td>\n",
       "      <td>1071.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>205.0</td>\n",
       "      <td>148.063415</td>\n",
       "      <td>141.953841</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.00</td>\n",
       "      <td>98.0</td>\n",
       "      <td>217.00</td>\n",
       "      <td>866.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>120.0</td>\n",
       "      <td>150.550000</td>\n",
       "      <td>118.675807</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.00</td>\n",
       "      <td>117.5</td>\n",
       "      <td>199.75</td>\n",
       "      <td>533.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>137.0</td>\n",
       "      <td>140.773723</td>\n",
       "      <td>109.399766</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.00</td>\n",
       "      <td>114.0</td>\n",
       "      <td>180.00</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>186.0</td>\n",
       "      <td>153.838710</td>\n",
       "      <td>119.103797</td>\n",
       "      <td>1.0</td>\n",
       "      <td>69.00</td>\n",
       "      <td>119.5</td>\n",
       "      <td>194.75</td>\n",
       "      <td>757.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>312.0</td>\n",
       "      <td>114.810897</td>\n",
       "      <td>104.439244</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.00</td>\n",
       "      <td>80.0</td>\n",
       "      <td>152.25</td>\n",
       "      <td>774.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>221.0</td>\n",
       "      <td>138.647059</td>\n",
       "      <td>137.051126</td>\n",
       "      <td>7.0</td>\n",
       "      <td>42.00</td>\n",
       "      <td>87.0</td>\n",
       "      <td>196.00</td>\n",
       "      <td>693.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocess_corpus(text):\n",
    "    \"\"\"\n",
    "    Before tokenization, normalize the text:\n",
    "\n",
    "        - Remove extra whitespace and line breaks.\n",
    "        - Convert text to lowercase (for case insensitivity).\n",
    "        - Remove non-alphanumeric characters (punctuation, special symbols).\n",
    "        - Normalize quotes and dashes.\n",
    "    :return: processed text\n",
    "    \"\"\"\n",
    "    text = clean_text(text)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    passages = [\" \".join(sentences[i:i + 5]) for i in range(0, len(sentences), 5)]\n",
    "    print(len(passages))\n",
    "    passages = [lemmatize(p) for p in passages]\n",
    "    return passages\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Before tokenization, normalize the text:\n",
    "\n",
    "        - Remove extra whitespace and line breaks.\n",
    "        - Convert text to lowercase (for case insensitivity).\n",
    "        - Remove non-alphanumeric characters (punctuation, special symbols).\n",
    "        - Normalize quotes and dashes.\n",
    "    :param text: text to process\n",
    "    :return: processed text\n",
    "    \"\"\"\n",
    "    # text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces and newlines\n",
    "    # text = re.sub(r'[“”‘’]', '\"', text)  # Normalize quotes\n",
    "    text = re.sub(r'[-—]', ' ', text)  # Normalize dashes\n",
    "    # text = re.sub(r'[^a-z0-9\\s]', '', text)  # Remove special characters\n",
    "    return text\n",
    "\n",
    "\n",
    "def lemmatize(text):\n",
    "    \"\"\"\n",
    "\n",
    "    :param text:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop])\n",
    "\n",
    "\n",
    "def create_chunks(text):\n",
    "    \"\"\"\n",
    "\n",
    "    :param text:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    passage_size = 5\n",
    "    passages = [\" \".join(sentences[i:i + passage_size]) for i in range(0, len(sentences), passage_size)]\n",
    "    return passages"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efeb047f349eee56",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def load_data(dir_path=\"data\", file_name=\"corcerers_stone_harry_poter1\", file_fmt=\"txt\"):\n",
    "    file_path = f\"{dir_path}/{file_name}.{file_fmt}\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        book_text = f.read()\n",
    "    return book_text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5622ff24f6193961",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "book_text = load_data()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74d0464c9f9f51ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "doc = nlp(book_text[:100])\n",
    "print([(w.text, w.pos_) for w in doc])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f0035945daec7e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Regular expression pattern for \"CHAPTER {NUMBER_IN_WORDS}\\n\\n{TITLE}\\n\\n\"\n",
    "chapter_pattern = r'\\n\\n\\nCHAPTER\\s+([A-Z]+)\\n\\n([A-Z\\s]+)\\n\\n'\n",
    "idx_title_tups = re.findall(chapter_pattern, book_text)\n",
    "chapters_titles = [m[1].strip() for m in idx_title_tups]\n",
    "\n",
    "chapter_pattern = r'\\n\\n\\nCHAPTER\\s+([A-Z]+)\\n\\n'\n",
    "chapters_idx_list = [f\"CHAPTER {idx}\" for idx in re.findall(chapter_pattern, book_text)]\n",
    "[txt.split(i) for i in chapters_idx_list]\n",
    "split_patterns = [f\"{c_idx}\\n\\n{c_title}\\n\\n\" for c_idx, c_title in zip(chapters_idx_list, chapters_titles)]\n",
    "chapters_idx_list"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bb66f158d6f9fe7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def clean_newlines(text):\n",
    "    text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be1f9c7090f3f568",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "store_csv = False\n",
    "\n",
    "book_text = load_data()\n",
    "chapter_pattern = r'(CHAPTER\\s+[A-Z]+)\\n\\n([A-Z\\s\\-]+)\\n\\n'\n",
    "sections = re.split(chapter_pattern, book_text)\n",
    "book_heading, book_content = sections[0], sections[1:]\n",
    "\n",
    "str_idx = book_content[0::3]\n",
    "chapters_titles = book_content[1::3]\n",
    "chapters_texts = book_content[2::3]\n",
    "chapter_idx = range(1, len(str_idx) + 1)\n",
    "\n",
    "book_df = pd.DataFrame.from_records(zip(chapter_idx, str_idx, chapters_titles, chapters_texts),\n",
    "                                    columns=['chapter_idx', 'str_idx', 'title', 'raw_content'])\n",
    "\n",
    "book_df['processed_content'] = book_df['raw_content'].apply(lambda chapter_content: clean_newlines(chapter_content))\n",
    "if store_csv:\n",
    "    book_df.to_csv('data/book_df.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec73d1fa82881445",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 3), max_features=500,stop_words='english')\n",
    "X = vectorizer.fit_transform(book_df['processed_content'])\n",
    "vectorizer.get_feature_names_out()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e57667169f1e708",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "book_df = pd.read_csv('data/book_df.csv')\n",
    "\n",
    "example_chapter = book_df['processed_content'].iloc[0]\n",
    "#doc = nlp(example_chapter)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0d64228587a3844",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from utils import persons_ents\n",
    "import thefuzz.utils\n",
    "from thefuzz import process\n",
    "from thefuzz import fuzz\n",
    "\n",
    "def get_match(person):\n",
    "    matches = process.extractOne(person, persons_ents, scorer=fuzz.token_set_ratio,\n",
    "                                 processor=thefuzz.process.default_processor)\n",
    "    if matches[1] < 90:\n",
    "        return None\n",
    "    return matches[0]"
   ],
   "id": "b8f9001c00285d04",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "persons_entities = set()\n",
    "persons_hist = {chapter:\"\" for chapter in book_df['str_idx'].to_list()}\n",
    "for i,c in enumerate(book_df['processed_content'].to_list()):\n",
    "    c = clean_text(c)\n",
    "    doc = nlp(c)\n",
    "    print(\"================================================================================\")\n",
    "    chapter = book_df['str_idx'].iloc[i]\n",
    "    persons_list = [entity.text for entity in doc.ents if entity.label_ == 'PERSON']\n",
    "    persons_set = set(persons_list)\n",
    "    print(\"Persons:\", persons_set)\n",
    "    for person in list(persons_set):\n",
    "        person_match = get_match(person)\n",
    "        print(f\"{person} match --> {person_match}\")\n",
    "        if person_match not in persons_hist.keys():\n",
    "            persons_hist[chapter][person_match] = 1\n",
    "        else:\n",
    "            persons_hist[chapter][person_match] += 1\n",
    "    # print(\"ORGs:\", set([entity.text for entity in doc.ents if entity.label_ == 'ORG']))\n",
    "    # print(\"PRODUCTs:\", set([entity.text for entity in doc.ents if entity.label_ == 'PRODUCT']))\n",
    "    # print(\"FACs:\", set([entity.text for entity in doc.ents if entity.label_ == 'FAC']))\n",
    "    persons_entities.add(set([entity.text for entity in doc.ents if entity.label_ == 'PERSON']))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c03d91b2c929750c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def tokenize(chapter):\n",
    "    sent_tokenize(chapter)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7288f69ce9f539cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2e11f215a974281"
  },
  {
   "cell_type": "code",
   "source": [
    "passage_idx = []\n",
    "\n",
    "for i, chapter_passages in enumerate(book_df['processed_content'].str.split('\\n\\n').to_list(), 1):\n",
    "    for j, single_passage in enumerate(chapter_passages):\n",
    "        passage_idx.append((i, j, single_passage))\n",
    "passages_df = pd.DataFrame.from_records(passage_idx, columns=['chapter_idx', 'passage_idx', 'content'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "841d1723704213ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "70b08fcec540a2a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "passages_df = pd.merge(left=passages_df, how='left', right=book_df[['str_idx', 'chapter_idx']], right_on='chapter_idx',\n",
    "                       left_on='chapter_idx')\n",
    "passages_df[['str_idx', 'chapter_idx', 'passage_idx', 'content']]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfb83764cf970dfb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "store_csv = False\n",
    "if store_csv:\n",
    "    passages_df.to_csv('data/passages_df.csv', index=False)"
   ],
   "id": "e5f85a7ce0079a82",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "query = [\"How does Harry find out he is a wizard?\", \"How was Harry's last month with the Dursleys?\"][1:]\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")  #, max_df=0.9, min_df=2)\n",
    "tfidf_matrix = vectorizer.fit_transform(book_df['processed_content'])\n",
    "query_vector = vectorizer.transform(query)\n",
    "\n",
    "similarity_scores = cosine_similarity(query_vector, tfidf_matrix)\n",
    "\n",
    "ranked_indices = np.argsort(similarity_scores[0])[::-1]\n",
    "print(ranked_indices)\n",
    "print(\"Top 5 Relevant Passages:\")\n",
    "for i in range(5):\n",
    "    idx = ranked_indices[i]\n",
    "    print(f\"\\nRank {i + 1}, Score: {similarity_scores[0][idx]:.4f}\")\n",
    "    print(book_df.iloc[idx, [0, 1, 2]])\n",
    "    print(book_df['processed_content'].iloc[-1])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3aa7741798ec4c99",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "chapters = book_df['processed_content'].tolist()\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text) \n",
    "    text = \" \".join([word for word in text.split() if word not in stopwords.words('english')])\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_top_words(model, feature_names, n_top_words=5):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        topics.append(\", \".join(top_words))\n",
    "    return topics\n",
    "\n",
    "processed_chapters = [preprocess_text(chapter) for chapter in chapters]\n",
    "chapter_topics = []\n",
    "for i, pc in enumerate(processed_chapters):\n",
    "    vectorizer = CountVectorizer(max_features=5000,ngram_range=(1,2)) \n",
    "    doc_term_matrix = vectorizer.fit_transform([pc])\n",
    "\n",
    "    lda_model = LatentDirichletAllocation(n_components=5, random_state=42)  # Extract 5 topics per chapter\n",
    "    lda_model.fit(doc_term_matrix)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    top5 = get_top_words(lda_model, feature_names)\n",
    "    print(book_df['title'].iloc[i])\n",
    "    print(top5)\n",
    "    chapter_topics.append(top5)\n",
    "    print()\n",
    "\n",
    "chapter_topics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b5698c63d8de7a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MINI LM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f383482a790c2847"
  },
  {
   "cell_type": "code",
   "source": [
    "def embedding_model():\n",
    "    EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fa1c71c5552e8c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def embedd_dataset(dataset,\n",
    "                   split: str = 'train',\n",
    "                   model: SentenceTransformer = SentenceTransformer('all-MiniLM-L6-v2'),\n",
    "                   text_field: str = 'content',\n",
    "                   rec_num: int = -1) -> tuple:\n",
    "    \"\"\"\n",
    "    Load a dataset and embedd the text field using a sentence-transformer model\n",
    "    :param dataset: The name of the dataset to load\n",
    "    :param split: The split of the dataset to load\n",
    "    :param model: The model to use for embedding\n",
    "    :param text_field: The field in the dataset that contains the text\n",
    "    :param rec_num: The number of records to load and embedd\n",
    "    :return: tuple: A tuple containing the dataset and the embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    embeddings = model.encode(dataset[text_field][:rec_num])\n",
    "    print(\"Done!\")\n",
    "    return dataset, embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9672b05a47bb2eb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Faiss index"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c372061e3200c61"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Unlike keyword-based search, semantic search uses the meaning of the search query. It finds relevant results even if they don’t exactly match the query. This works by combining the power of Large Language Models (LLMs) to generate vector embeddings with the long-term memory of a vector database.\n",
    "\n",
    "Once the embeddings are stored inside a vector database like Pinecone, they can be searched by semantic similarity to power applications for a variety of use cases"
   ],
   "id": "632ae61cf5919a9"
  },
  {
   "cell_type": "code",
   "source": [
    "def build_faiss_flatl2_index(\n",
    "        index_vectors: np.ndarray,\n",
    "        dim: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function builds a Faiss flat L2 index.\n",
    "    Args:\n",
    "        index_vectors: An array of shape (n_index, dim) containing the index vectors.\n",
    "        dim: The dimensionality of the vectors. \n",
    "    Returns:\n",
    "        A Faiss flat L2 index.\n",
    "    \"\"\"\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "def build_faiss_lsh_index(\n",
    "        index_vectors: np.ndarray,\n",
    "        dim: int,\n",
    "        nbits: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function builds a Faiss LSH index.\n",
    "    Args:\n",
    "        index_vectors: An array of shape (n_index, dim) containing the index vectors.\n",
    "        dim: The dimensionality of the vectors. \n",
    "        nbits: The number of bits to use in the hash.\n",
    "    Returns:\n",
    "        A Faiss LSH index.\n",
    "    \"\"\"\n",
    "    index = faiss.IndexLSH(dim, nbits)\n",
    "    index.add(index_vectors)\n",
    "    return index\n",
    "\n",
    "def train_index(index, sentence_embeddings: np.ndarray):\n",
    "    index.add(sentence_embeddings)\n",
    "\n",
    "def encode_query(query: str):\n",
    "    q_embd = model.encode(query)\n",
    "    return q_embd\n",
    "\n",
    "\n",
    "def compute_recall_at_k(\n",
    "        nn_gt: np.ndarray,\n",
    "        ann: np.ndarray,\n",
    "        k: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function computes the recall@k.\n",
    "    Args:\n",
    "        nn_gt: The ground truth nearest neighbors.\n",
    "        ann: The approximate nearest neighbors.\n",
    "        k: The number of nearest neighbors to consider.\n",
    "    Returns:\n",
    "        The recall@k.\n",
    "    \"\"\"\n",
    "    return round(sum([len(set(ann[i]) & set(nn_gt[i])) / k for i in range(len(ann))])/len(ann), 3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2bcdf1acd26cb780",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model = embedding_model()\n",
    "dataset, embeddings = embedd_dataset(\n",
    "    dataset=passages_df,\n",
    "    rec_num=-1,\n",
    "    model=model,\n",
    ")\n",
    "embeddings_shape = embeddings.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f67573bb6189d16",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "sentences = passages_df.content.to_list()\n",
    "# Define number of clusters (adjust dynamically based on the chapter length)\n",
    "num_clusters = len(sentences) // 7  # Roughly one cluster per 5 sentences\n",
    "\n",
    "\n",
    "clustering_model = AgglomerativeClustering(n_clusters=num_clusters,metric='cosine', linkage='average')\n",
    "cluster_labels = clustering_model.fit_predict(embeddings)\n",
    "\n",
    "\n",
    "sentence_clusters = list(zip(sentences, cluster_labels))\n",
    "cluster_dict = defaultdict(list)\n",
    "for sentence, cluster in sentence_clusters:\n",
    "    cluster_dict[cluster].append(sentence)\n",
    "\n",
    "\n",
    "content_chunks = [\" \".join(cluster_dict[cluster]) for cluster in sorted(cluster_dict.keys())]\n",
    "\n",
    "print(f\"Generated {len(content_chunks)} content-based chunks.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "334110192afd02d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=cluster_labels, cmap='Spectral', alpha=0.7)\n",
    "plt.title(\"Semantic Clustering of Sentences\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.colorbar(label=\"Cluster ID\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f43f2d8b2222988",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "index = build_faiss_flatl2_index(embeddings,embeddings_shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41f0d58dbe311c32",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Reduce dimensionality of embeddings to visualize in 2D\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "# Plot the 2D projection of vectors\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], alpha=0.5)\n",
    "plt.title(\"PCA Projection of Passage Embeddings\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6663db42e990eed6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### We can observe a well-distributed spread of points, indicating a good semantic separation. We can see 4 clusters, which inddicate similar thematic content in the text."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c408b36c769c6073"
  },
  {
   "cell_type": "code",
   "source": [
    "def retrieve_top_passages(query, model, index, chunks, top_n=5):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, top_n) \n",
    "    results = [(chunks[idx], distances[0][i]) for i, idx in enumerate(indices[0])]\n",
    "    return results\n",
    "\n",
    "# Define queries\n",
    "queries = [\n",
    "    \"What is Hogwarts?\",\n",
    "    \"Who is Harry Potter?\",\n",
    "    \"How does magic work?\"\n",
    "]\n",
    "\n",
    "\n",
    "retrieval_results = {query: retrieve_top_passages(query, model, index, chunks) for query in queries}\n",
    "\n",
    "retrieval_data = []\n",
    "for query, passages in retrieval_results.items():\n",
    "    for rank, (text, score) in enumerate(passages):\n",
    "        retrieval_data.append({\"Query\": query, \"Rank\": rank + 1, \"Score\": score, \"Passage\": text[:200] + \"...\"})  \n",
    "\n",
    "retrieval_df = pd.DataFrame(retrieval_data)\n",
    "retrieval_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12df9152cfaab74d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
